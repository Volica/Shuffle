
spark统一安装在服务器的/tmp/spark目录下，机房经常停电，重启服务器可能会消失。所以最好在项目验收之前每天手动备份一下，防止配置文件丢失，还要重新配置。
```
cp -r /tmp/spark /home/djk/spark
```

下载模版代码
https://github.com/JiankunDai/shuffle-experiment


打包方式
```
cd shuffle-experiment
sbt assembly
```

提交方式
```
spark-submit \
  --class edu.ecnu.ShuffleExperiment \
  --master spark://49.52.27.49:7077 \
  --deploy-mode cluster \
  --executor-memory 1G \
  --driver-memory 512M \
  --executor-cores 2 \
  --conf spark.executor.instances=4 \
  --conf spark.dynamicAllocation.enabled=false \
  --total-executor-cores 8 \
  --conf spark.sql.adaptive.enabled=false \
  --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
  file:///home/djk/Documents/shuffle-experiment-sort/target/scala-2.10/spark-shuffle-experiment-1.0.0.jar
```
注意将命令最末尾替换为实际打包生成的jar文件地址。
如果有java.io.FileNotFoundException错误堆栈输出就重新提交。
没有错误输出就可以在WebUI看到Running Applications了

从Workers列表点进去可以看stdout和stderr

spark-webUI的地址：http://49.52.27.49:8080/
任意节点提交任务后可以在这里看任务执行情况


需要关注的配置文件:
/tmp/spark/conf/spark-env.sh # spark环境变量相关


